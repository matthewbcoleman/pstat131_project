---
title: "Airbnb Project"
author: "Matthew Coleman, Austin Mac, Jeff Pittman, and Nick Reyes"
date: "2/28/2020"
output: 
  pdf_document:
    number_sections: true
    fig_caption: yes
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
   
---

```{r global_options, include=FALSE}
library(knitr)
knitr::opts_chunk$set(fig.pos = 'H')
# knitr::opts_chunk$set(fig.pos = '!h')
#includes:  
# in_header: preamble-latex.tex
```

```{r library, include = F}
# Anything below here before the abstract has to be here so we can run code in the analysis and have output
library(tidyverse)
library(dplyr)
library(randomForest)
library(class)
library(tree)
library(gbm)
library(caret)
library(rpart.plot)
library(rattle)
library(knitr)
library(fastAdaboost)
library(ggpubr)
library(MASS)
library(kableExtra)
library(glmnet)
library(faraway)
library(ROCR)
#library(train)
```

```{r read_data, echo = F}
bnb <- read.csv('AB_NYC_2019.csv')

bnb <- as_tibble(bnb)

dims <- dim(bnb)

#sprintf('Our dataset has %d observations and %d attributes', dims[1],dims[2])
# [1] "Our dataset has 48895 observations and 16 attributes"
```

```{r remove_zero, echo = F}
bnb <- bnb[(bnb$price!=0),]
```

```{r train-test, echo = F}
set.seed(123)
train.ind <- sample(1:nrow(bnb),size = .8*nrow(bnb))
small.ind <- sample(1:nrow(bnb), size = .15*nrow(bnb))


train.big <- bnb[train.ind,]
test.big <- bnb[-train.ind,]

small <- bnb[small.ind,]
train.small <- sample(1:nrow(small), size = .8*nrow(small))
train <- small[train.small,]
test <- small[-train.small,]

```

```{r na_cols, echo = F}
#colnames(train)[apply(train, 2, anyNA)]
```

```{r sum_zero, echo = F}
#with(train, sum((is.na(reviews_per_month)) & (number_of_reviews!=0)) )
# [1] 0
```

```{r set_na, echo = F}
train[is.na(train$reviews_per_month),'reviews_per_month'] <- 0
test[is.na(test$reviews_per_month),'reviews_per_month'] <- 0

#sum(is.na(train$reviews_per_month))
# [1] 0
#sum(is.na(train$reviews_per_month))
# [1] 0
```

```{r price_above, echo = F}
train$price_above <- ifelse(train$price> median(train$price),1,0)
#train$price_above <- as.factor(train$price_above)
test$price_above <- ifelse(test$price> median(test$price),1,0)
#test$price_above <- as.factor(test$price_above)
```

```{r pariplot1, echo = F}
num.feat <- train %>% dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count, price) 
num.feat$lprice <- log(num.feat$price)
```

```{r pairplot2, include = F}
pairs(num.feat)
```

```{r neighbourhood, eval = F}
# levels(bnb$room_type)
# 
# levels(bnb$neighbourhood_group)
# 
# n_distinct(bnb$neighbourhood)
# 
# n_distinct(bnb$neighbourhood_group)

# [1] "Entire home/apt" "Private room"    "Shared room"    
# [1] "Bronx"         "Brooklyn"      "Manhattan"     "Queens"        "Staten Island"
# [1] 221
# [1] 5
```

# Abstract

# Introduction
This project was conducted to predict the prices of Airbnb rentals in New York City. We aimed to compare parametric and non-parametric supervised machine learning models to select the ones with the lowest MSE and misclassification rates. 


# Methods

## Data

The dataset we will be using for our analysis is the dataset [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) from Kaggle. This dataset contains the listing activity and metrics for Airbnb in New York City, New York during 2019. There are 48895 observations and 16 attributes to the dataset. The main features we are going to use for our analysis include the following:

* Price: Our main response variable. The price, in dollars, of the listing per night. Log-transformed to normalize distribution. 

```{r logprice_hist, echo = FALSE, fig.cap = 'Log-transformed Price'}
hist(log(train$price), main='Histogram of log(Price)', xlab = 'log(Price)')
```

* Price Above: Variable created from `price`, `price_above` is a binary variable of signaling whether a listings price is above the median listing price. 1 represents the price being above the median, and 0 represents the price being below the median.
* Neighbourhood: Categorical variable of the neighbourhood to which a listing belongs. This is a nested version of neigbhourhood group, with 221 unique neighbourhood groups.
* Neighbourhood Group: Factor variable of the neighbourhood group to which the listing belongs. There are 5 neighbourhood groups in the dataset.
  * Plots of both neigbourhood and neighbourhood group are shown below: 
  
    ```{r location, fig.width = 6.5, echo = F, fig.cap = 'Neighbourhood and neighbourhood group'}
n <- ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood)) + 
  geom_point() + theme(legend.position="none") + xlab('Latitude') + ylab('Longitude') +
  ggtitle('Neighbourhood')

ng <- ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood_group)) + 
  geom_point() + theme(legend.position="none") + xlab('Latitude') + ylab('Longitude') +
  ggtitle('Neighbourhood Groups')

fig <- ggarrange(n, ng, nrow = 1, ncol = 2)

annotate_figure(fig, top = 'New York City Airbnb Listing Locations By:')
```


```{r mean_price, echo = F, fig.cap = 'Justification for using neighbourhood group'}
mean_price <- train %>% group_by(neighbourhood) %>% summarise(mean_price = mean(price), 
                                                            latitude = median(latitude), longitude = 
                                                            median(longitude)) 

#plot(mean_price$latitude, mean_price$longitude, col = mean_price$mean_price)

ggplot(data = mean_price, aes(x = latitude, y = longitude, color = mean_price)) + 
  geom_point() + scale_color_gradient(low="blue", high="red", name = 'Mean Price (USD)') + xlab('Latitude') + 
  ylab('Longitude') + ggtitle('Mean Price by Neighborhood')
```

* Latitude: Latitude coordinates of the listing.
* Longitude: Longitude coordinates of the listing.
* Room Type: The listing space type. Three types: *Entire home/apt*, *Private room*, *Shared room*. 
* Minimum Nights: The mininum amount of nights someone can stay in the listing.
* Number of reviews: The number of reviews for the host.
* Reviews per Month: The number of reviews per month for the host. Formula:$\frac{Number\ of\ Reviews}{Months\ Listed}$.
* Calculated Host Listings Count: The number of listings per host.

All atributes were complete with the exception of `last_review`, which has the date of the last review, and `reviews_per_month`. Upon further exploration, the reviews per month feature was NA only when the host had no reviews. This resulted in us imputing 0's for NA values in the reviews per month column. Because the date of last review was unimportant to our analyses, we did not impute values for this column.

### Assumptions.

Many of our machine learning methods are very computationally intensive, so we sampled 15% of the entire dataset, and then train-test split the 15% sample into 80% training 20% test dataset. To verify this was a viable practice, we plotted the distribution of our response variable, price, and verified the distribution is the similar to the distribution of the overall dataset. The histogram is very similar, and even contains some of the outliers we can see in the overall dataset, so we assumed our smaller dataset was representative of the population.

```{r price_hist, echo = F, fig.cap= 'Training Data Histogram'}
par(mfrow = c(1,2))
hist(bnb$price, main='Histogram of Overall Price', xlab = 'Price (USD)')
hist(train$price, main='Histogram of Training Price', xlab = 'Price (USD)')
```

We assessed the correlation between our variables with a correlation heatmap:

```{r corr_plot, echo = F, fig.cap = 'Feature Correlations'}
num.feat <- train %>% dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count, price) 
num.feat$lprice <- log(num.feat$price)
feat.corr <- cor(num.feat)
corrplot::corrplot(feat.corr)
```

Reviews per month and number of reviews were highly correlated, so we decided to remove number of reviews to account for collinearity.

### Sample Sizes

Our overall dataset is 48895. Taking the proposed 15% split on the data left us with an overall dataset of 7332 observations. The 80/20 train-test split left us with 5865 training samples and 1467 test samples.

## Machine Learning Methods

### Regression Methods

Methods used to predict the price of a listing:

* Ridge Regression:

  * Constraint optimization on the least squares criterion:
  
$$\hat{\beta}_{ridge} = \underset{\beta}{argmin}[||Y-XB||^2 + \lambda\sum_{j=1}^p\beta_j^2] $$

* Lasso Regression: 

  * Constraint optimization and model selection on the least squares criterion:
  
$$\hat{\beta}_{ridge} = \underset{\beta}{argmin}[\ ||Y-XB||^2 + \lambda\sum_{j=1}^p|\beta_j|\ ]$$

By using these two methods, we can try to reduce our estimates for the linear model by imposing some Bias on our estimates for $\beta$. Another benefit of using Lasso regression is that we can also perform model selection, making a simpler model.

* Tree Methods

  * Individual Trees: To compare the efficacy of ensemble tree methods, we will fit an individual regression tree on longitude and latitude, and then one tree on all variables of interest.
  
  * Bagging: We will fit an ensemble tree method which will grow large trees on bootstrapped data, resulting in high variance low bias. All of these trees predictions will be averaged to give the final prediction.
  
  * Random Forest: We will create multiple decision trees similar to bagging, but try to decorrelate each of the bootstrap trees through selecting $m=\frac{p}{3}$ variables. 
  
  * Boosting: We will fit multiple (weak) trees sequentially, grown on information from the previously grown tree. Final prediction is a weighted prediction of the weak learners.
  

### Classification Methods

Methods used to predict whether a listings price is above the median:

* Logistic Regression: We will fit a logistic regression model on all variables of interest,using a binary classification output to predict whether a listing's price is above or below the median.
* LDA
* QDA

* Tree Methods

  * Individual Trees: To compare the efficacy of ensemble tree methods, we will fit an individual classification tree on longitude and latitude, and then one tree on all variables of interest.
  
  * Bagging: We will fit an ensemble tree method which will grow large trees on bootstrapped data, resulting in high variance low bias. All of these trees predictions will be chosen by majority voting for the final prediction.
  
  * Random Forest: We will create multiple decision trees similar to bagging, but try to decorrelate each of the bootstrap trees through selecting $m=\sqrt{p}$ variables. Final predictions will be through majority voting. 
  
  * Boosting: We will fit multiple (weak) trees sequentially, grown on information from the previously grown tree. Final prediction is a weighted of the weak learners 
  
* SVM: We will fit support vector machines with different kernels (Linear, Polynomial, Radial). In order to select the best possible support vector machines, we will use k-fold cross validation to tune the cost parameter to obtain the lowest misclassification rate. 

* KNN: We will fit a K-Nearest Neighbours model with optimal K selected by cross validation. 


# Analysis and Discussion

## Linear Models

```{r ols, echo = F}
set.seed(123)
ols.price <- lm(log(price) ~ latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)
ols.pred <- predict(ols.price, test)

ols.mspe <- mean((log(test$price)-ols.pred)^2)
ols.mspe

#summary(ols.price)
```

```{r logit, echo = F}
#initial LR model
set.seed(123)
logit.fit <- glm(price_above ~longitude + latitude+ minimum_nights+ calculated_host_listings_count+ 
                   availability_365 + reviews_per_month + room_type + neighbourhood_group, data=train,
                 family=binomial('logit'))

fit.pred <- predict(logit.fit, test, type="response")
fit.pred <- ifelse(fit.pred>.5, 1, 0)
table(fit.pred,test$price_above)
mean(fit.pred!=test$price_above)

pred <- prediction(fit.pred, test$price_above)
perf <- performance(pred, "tpr","fpr")
plot(perf, col=2, lwd=3, main="ROC curve")
abline(0,1)

auc = performance(pred, "auc")@y.values

```

## Discriminant Analysis

```{r lda, echo = F}
#initial lda model
lda.fit <- lda(price_above ~ 
                 longitude 
               + latitude
               + minimum_nights
               + calculated_host_listings_count
               + availability_365
               + reviews_per_month
               + room_type
               + neighbourhood_group
               ,
               data=train)
lda.fit

yhat <- predict(lda.fit, test)$class
tr.tbl <- table(obs=test$price_above,pred=yhat)
tr.tbl
mean(yhat != test$price_above)
```

``` {r qda, echo = F}
qda.fit <- qda(price_above ~ longitude + latitude
               + minimum_nights
               + calculated_host_listings_count
               + availability_365
               + reviews_per_month
               + room_type
               + neighbourhood_group
               ,
               data=train)
qda.fit

yhat <- predict(qda.fit)$class
tr.tbl <- table(obs=train$price_above,pred=yhat)
tr.tbl
1-sum(diag(tr.tbl))/sum(tr.tbl)
```

## Tree-based Methods

### Classification and Regression Trees

```{r class_tree, echo = F, fig.cap = 'Classification Tree'}
set.seed(123)
class.tree <- rpart(as.factor(price_above)~latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)

tree.class.prediction <- predict(class.tree, test, type = 'class')
tree.class.misclass <- mean(test$price_above !=tree.class.prediction)

fancyRpartPlot(class.tree, digits = 6, main = 'Classification Tree for Price Above Median', sub = '')
```

The classification tree on all predictors split only on the type of room. We can see from the dendrogram that if a room is a private room or a shared room, the listing would be classified as "below the median price," and if it is a whole apartment or home then it would be classified as "below the median price."

```{r prune_tree, echo = F}
set.seed(123)

#Pruning tree did not improve tree

# prune <- prune.tree(loc.tree, best = 4, newdata = test)
# plot(prune)
# text(prune)
# tree.class.misclass
```

```{r reg_tree, echo = F, fig.cap= 'Regression Tree',fig.height = 4, fig.width = 6}
set.seed(123)
tree.reg <- rpart(log(price) ~ latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)

reg.prediction <- predict(tree.reg, test)
tree.mspe <- mean((log(test$price)-reg.prediction)^2)

fancyRpartPlot(tree.reg, digits = 6,  sub = '', main = 'Regression Tree for log(Price)')
```

The regression tree on is more intricate than the classification tree. The main split is on the room type of the listing, and then the next two splits are made on the neighbourhood group. The last splits made are on the location of the the listing.

### Random Forests

```{r rf_class, echo = F}
set.seed(123)
rf.class <- randomForest(as.factor(price_above)  ~ latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

#randomForest::importance(rf.class)

rf.class.pred <- predict(rf.class,test)
rf.misclass <- mean(test$price_above!=rf.class.pred)

#sprintf('The misclassification rate for the classification random forest is %f', rf.misclass)
#varImpPlot(rf.class)
```

```{r rf_reg, echo = F}
set.seed(123)
rf.reg <- randomForest(log(price) ~ latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

rf.reg.pred <- predict(rf.reg,test)
rf.mspe <- mean((log(test$price)-rf.reg.pred)^2)

#sprintf('The mean squared prediction error for the regression random forest is %f', rf.mspe)
```

```{r var_imp, echo = F, fig.width = 7, fig.height=7, fig.cap = 'Variable Importances for RF Models'}
par(mfrow = c(2,1))
varImpPlot(rf.class, main = 'RF Classification Model')
varImpPlot(rf.reg, main = 'RF Regression Model')
```

The random forest model imporances showed that room type, longitude, latitude, and reviews per month were the most important variables for the decrease in gini impurity for classification forests. For the regression model, room type, longitude, latitude, and neighbourhood group were the most important variables for the increase in node purity.

```{r cv_rf, echo = F, fig.cap = 'Cross Validation Error by # of Predictors'}
set.seed(123)
rf.cv.trainx <- train %>% dplyr::select(latitude, longitude, minimum_nights, reviews_per_month,
                                neighbourhood_group, room_type, calculated_host_listings_count)
rf.cv.trainy <- log(train$price)
cv.rf <- rfcv(rf.cv.trainx,rf.cv.trainy, cv.fold = 5)
plot(cv.rf$n.var, cv.rf$error.cv, type = 'b', xlab = 'Number of Variables in Model', ylab = 'Cross-Validation Error')
```

Through cross validation, we were able to determine 4 variables in the model leads to the greatest decrease in the cross-validation error while accounting for model complexity. To choose the 4 variables to use in the reduced random forest model, I used the criteria of greatest variable importance from above. This means we chose room type, longitude, latitude, and reviews per month for the classification model and room type, longitude, latitude, and neighbourhood group of a regression model.

```{r red_rf, echo = F}
set.seed(123)
rf.reg.reduced <- randomForest(log(price) ~ latitude + longitude + neighbourhood_group + room_type, 
                               data = train, importance = TRUE)

rfr.reg.pred <- predict(rf.reg.reduced,test)
rfr.reg.mspe <- mean((log(test$price)-rfr.reg.pred)^2)

#sprintf('The mean squared prediction error for the reduced regression random forest is %f', rf.red.mspe)
```

```{r red_class_rf, echo = F}
set.seed(123)

rf.class.reduced <- randomForest(as.factor(price_above)  ~latitude + longitude +  reviews_per_month +
                       room_type , data = train, importance = TRUE)

rfr.class.pred <- predict(rf.class.reduced,test)
rfr.misclass <- mean(test$price_above!=rfr.class.pred)

#sprintf('The misclassification rate for the reduced classification random forest is %f', rfr.misclass)
```

The misclassification rate for the random forest model with all the predictors included was `r rf.misclass`, as opposed to the `r rfr.misclass` misclassification rate of the reduced model. While the RF model with all the predictors is more accurate than the smaller model, the smaller model is simpler and more likely to be scalable in different scenarios. The mean squared prediction error of the full model, `r rf.mspe`, is also lower than the `r rfr.reg.mspe` MSPE of the smaller model. As with the classification forest, the simpler model is more scalable at the cost of prediction error. Another downside of the larger model is the possibility of overfitting to the training dataset.

### Boostrap-Aggregating (Bagging)

```{r bag_reg, echo = F}
set.seed(123)
bag.reg <- randomForest(log(price)  ~ latitude + longitude + reviews_per_month +  room_type,
                     data = train, mtry = 4 , importance = TRUE)

bag.reg.pred <- predict(bag.reg,test)
bag.mspe <- mean((log(test$price)-bag.reg.pred)^2)

# sprintf('The mean squared prediction error for bagging is %f', bag.mspe)
```

```{r bag_class, echo = F}
set.seed(123)
bag.class <- randomForest(as.factor(price_above) ~ latitude + longitude + reviews_per_month +  room_type,
                     data = train, mtry = 4 , importance = TRUE)

bag.class.pred <- predict(bag.class,test)
bag.misclass <- mean(test$price_above!=bag.class.pred)

# sprintf('The misclassification rate for the bagged classification model is %f', bag.misclass)
```

The bagging model was similar to the random forest model with a misclassification rate of `r bag.misclass` and a MSPE of `r bag.mspe`.

### Boosting

```{r boost_red, echo = F}
set.seed(123)
boost.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, cv.folds = 5, distribution = 'gaussian')
boost.pred <- predict(boost.mod, test, n.trees = 1000)

boost.mspe <- mean((log(test$price)-boost.pred)^2)

#sprintf('The mean squared prediction error for boosting is %f', boost.mspe)
```

The first parameter we tuned in the boosting model was the shrinkage parameter, $\lambda$. The lambda which results in the lowest MSPE will be the lambda used in the final boosting model.

```{r boost_lambda, echo = F}
set.seed(123)
lambdas <- seq(0,.15, .002)
b.mspe.list <- NULL

for(lambda in lambdas){
  boost.l.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, shrinkage = lambda, distribution = 'gaussian')
  boost.l.pred <- predict(boost.l.mod, test, n.trees = 1000)

  b.mspe.list <- append(b.mspe.list,mean((log(test$price)-boost.l.pred)^2))
}
plot(lambdas,b.mspe.list, type = 'b', ylab = 'Boosted MSPE', xlab = 'Lambda', 
     main = 'MSPE vs. Lambdas')
```

There was not a discernable optimal lambda for test MSE, so we decided to use the default value of $\lambda = 0.1$. This made model parameter selection the easiest. 

```{r best_boost, echo = F}
set.seed(123)
best.lambda <- lambdas[which.min(b.mspe.list)]

best.boost <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, distribution = 'gaussian')
best.boost <- predict(boost.mod, test, n.trees = 1000)

b.boost.mspe <- mean((log(test$price)-best.boost)^2)

#sprintf('The mean squared prediction error for bagging with the optimal lambda is is %f', b.boost.mspe)
```

```{r boost_class_lambda, echo = F }
set.seed(123)
lambdas <- seq(0,.15, .002)
b.mis.list <- NULL

for(lambda in lambdas){
  boost.m.mod <- gbm(as.character(price_above) ~ latitude + longitude +
                       reviews_per_month +  room_type, data = train,
                     n.trees = 1000, shrinkage = lambda, distribution = 'bernoulli')
  boost.m.pred <- predict(boost.m.mod, test, n.trees = 1000, type = 'response')
  boost.m.pred <- ifelse(boost.m.pred>=.51, 1,0)

  b.mis.list <- append(b.mis.list, mean(test$price_above!=boost.m.pred))
}
plot(lambdas, b.mis.list, type = 'b', ylab = 'Boosted Misclass', xlab = 'Lambda',
     main = 'Misclassification. vs. Lambdas')

best.class.lambda = 0.1

```

As with the regression boosting model, the misclassification error rate does not seem to be at a minimum for any value of lambda, so we will also choose the default value for $\lambda$.

The other parameter tuned for the boosing model was the number of trees used.

```{r ntree, echo = F}
set.seed(123)
tree.err <- NULL

ntrees <- seq(250,2500,250)

for(ntree in ntrees){
  boost.t.mod <- gbm(log(price) ~ latitude + longitude + 
                       reviews_per_month +  room_type, data = train, n.trees = ntree, 
                     shrinkage = best.lambda, distribution = 'gaussian')
  boost.t.pred <- predict(boost.t.mod, test,n.trees = ntree)

  tree.err <- append(tree.err,mean((log(test$price)-boost.t.pred)^2))
}
#tree.err

plot(ntrees, tree.err, type = 'b', ylab = 'Boosted MSPE', xlab = 'Trees', 
     main = 'MSPE vs. Number of Trees')
best.tree <- ntrees[which.min(tree.err)]
```

The MSPE was the lowest for the model with 1500 trees, so we implemented this into the final boosting model.

```{r end_class_boost, echo = F}
set.seed(123)

# class.boost <- train(as.factor(price_above) ~ latitude + longitude + reviews_per_month +  room_type, 
#                      method = 'gbm', data = train, verbose = FALSE)

class.boost <- gbm(as.character(price_above) ~ latitude + longitude + reviews_per_month +  room_type,
                   n.trees = best.tree, data = train, distribution = 'bernoulli')


boost.class.pred<- predict(class.boost, test, n.trees = 1000, type = 'response')
boost.class.pred <- ifelse(boost.class.pred>=.51, 1,0)


boost.misclass <- mean(test$price_above!=boost.class.pred)
#boost.misclass
```

```{r end_reg_boost, echo = F}
set.seed(123)
boost.reg.final <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000,  distribution = 'gaussian')
boost.pred <- predict(boost.reg.final, test, n.trees = 1000)

boost.mspe <- mean((log(test$price)-boost.pred)^2)

#sprintf('The mean squared prediction error for boosting is %f', boost.mspe)
```

### Tree Method Summary

All The tables for tree methods put together. This will allow us to evaluate the efficacy of our tree models and determine which is the best.

```{r tree_tables, echo = FALSE}
Methods <- c('Tree','Bagging','Boosting', 'Random Forest', 'Reduced Random Forest')
MSPE <- round(c(tree.mspe, bag.mspe, boost.mspe, rf.mspe, rfr.reg.mspe),6)
Misclassification <- round(c(tree.class.misclass, bag.misclass ,boost.misclass, rf.misclass, rfr.misclass), 6)

mspe.tab  <- cbind(Methods,MSPE)
misclass.tab <- cbind(Methods,Misclassification)

kable(list(`MSPE` = mspe.tab, Misclassification  = misclass.tab), 
      caption = 'Error Rates for Tree Models') %>% kable_styling(latex_options = "hold_position")
```

The table with all the tree method error rate shows there is not a clear "best model" for the data. The random forest with all predictors has the lowest test MSE and misclassification error rate, but suffers from the possibility of overfitting the data and being too complex of a model. Because all of the methods are approximately equal in terms of predictive performance, a simpler model such as a simple tree or the reduced random forest may be best. Both of these models are simpler, and would therefore be more scalable in larger-data environments.

## SVM

In order to gain a rudimentary understanding of our data, we plotted a scatterplot Airbnb properties in New York coded by price above or below the median. It appears that the price of properties is noticably higher along waterfront properties. 
```{r}
library(e1071)
train$price_above <- as.factor(train$price_above)

plot(rbind(train, test)$latitude, 
     rbind(train,test)$longitude, 
     col = rbind(train, test)$price_above, 
     main = "Price by Location",
     xlab = "Latitude",
     ylab = "Longitude")
```


### Best Linear Kernel SVM
* Misclass. Rate: 0.1867757
* Cost Parameter: 0.06
* Support Vectors: 4598

Below is a preliminary plot of a linear kernel SVM model fit off of latitude and longitude. After including other predictors including `neighbourhood_group`, `minimum_nights`, `room_type`, `number_of_reviews`, `calculated_host_listings_count`, `availability_365`, and using 10-fold cross validation to select the best cost parameter, we obtain a cost parameter with value 0.06. This gives us a test misclassification rate of 0.1867757.
```{r, echo=FALSE}
# determine approximate best cost parameter
# tune.linear <- tune(svm, price_above ~ latitude+longitude, data = train, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
# increase precision of cost parameter
# tune.linear <- tune(svm, price_above ~ longitude
#                     +latitude
#                     +neighbourhood
#                     +minimum_nights
#                     +room_type
#                     +minimum_nights
#                     +number_of_reviews
#                     +calculated_host_listings_count
#                     +availability_365, data = train, kernel = "linear", ranges = list(cost = seq(.05, .15, .01)))
plot(svm(price_above ~ longitude+latitude, data = train, kernel = "linear", cost = 0.06), test[,c("price_above","longitude", "latitude")])
best.linear <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood_group
                    +minimum_nights
                    +room_type
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "linear", cost = 0.06)
pred.linear <- predict(best.linear, test)

# confusion matrix
conf.linear <- table(obs = test$price_above, pred = pred.linear)
acc.linear <- 1 - sum(diag(conf.linear)/sum(conf.linear))

```

### Best Polynomial Kernel SVM
* Misclass. Rate: 0.1724608
* Cost Parameter: 100
* Degree: 3
* Support Vectors: 5737

Below is a plot of a polynomial kernel SVM decision boundary fit off of `longitude` and `latitude`. Fitting the model with identical predictors as our linear kernel SVM and using 10-fold cross validation, we obtain an optimal cost parameter value of 100. Our test misclassification rate is 0.1724608
```{r, echo=FALSE}
# tune.poly <- tune(svm, price_above ~ longitude
#                     +latitude
#                     +neighbourhood_group
#                     +minimum_nights
#                     +room_type
#                     +minimum_nights
#                     +number_of_reviews
#                     +calculated_host_listings_count
#                     +availability_365, data = train, kernel = "polynomial", ranges = list(cost = c(100, 1000, 10000)))

plot(svm(price_above ~ longitude+latitude, data = train, kernel = "polynomial", cost = 100), test[,c("price_above", "longitude", "latitude")])
best.poly <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood_group
                    +minimum_nights
                    +room_type
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "polynomial", cost = 100)
pred.poly <- predict(best.poly, test)
# (MSE.poly <- mean((as.numeric(test$price_above) - as.numeric(pred.poly))^2))

# confusion matrix
conf.poly <- table(obs = test$price_above, pred = as.factor(pred.poly))
acc.poly <- 1 - sum(diag(conf.poly)/sum(conf.poly))
```

### Best Radial Kernel SVM
* Misclass. Rate: 0.1785958
* Cost Parameter: 8
* Support Vectors: 3615
Below is a plot of the decision boundary of a radial kernel SVM fit on `latitude` and `longitude`. Fitting a model with identical parameters as the previous two models, we obtain a cost parameter value of 8 through 10-fold cross validation. Our test misclassification rate is 0.1785958.
```{r, echo=FALSE}
# tune.rad <- tune(svm, price_above ~ longitude+latitude, data = train, kernel = "radial", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
# tune.rad <- tune(svm, price_above ~ longitude
#                     +latitude
#                     +neighbourhood_group
#                     +minimum_nights
#                     +room_type
#                     +number_of_reviews
#                     +calculated_host_listings_count
#                     +availability_365, data = train, kernel = "radial", ranges = list(cost = seq(8, 11, 1)))
plot(svm(price_above ~ longitude+latitude, data = train, kernel = "radial", cost = 8), test[,c("price_above", "latitude", "longitude")])
best.rad <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood_group
                    +minimum_nights
                    +room_type
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "radial", cost = 8)
pred.rad <- predict(best.rad, test)
conf.rad <- table(obs=test$price_above, pred=pred.rad)
acc.rad <- 1 - sum(diag(conf.rad)/sum(conf.rad))
```


## KNN
Using cross validation to find k such that the misclassification rate is minimized, we find that k = 27 gives us the minimum misclassification rate of 0.1642808. In the plot below, accuracy is maximized at k = 27.

```{r}
set.seed(3)
knn.caret <- train(price_above ~ neighbourhood_group+latitude+longitude+room_type+minimum_nights+number_of_reviews+calculated_host_listings_count+availability_365, data = train, method = "knn", trControl = trainControl("cv", number = 10), preProcess = c("center","scale"), tuneLength = 20)
plot(knn.caret)
```

```{r}
pred.knn <- predict(knn.caret, test)

conf.matrix <- table(pred = pred.knn, obs = test$price_above)
acc.knn <- 1 - sum(diag(conf.matrix))/sum(conf.matrix)
```

## Regularization

###Ridge Regression
```{r glmnet}


X <- data.matrix(num.feat)
#X <- X[,-c(10,2,4,5,6,9,13,14)]
y <- log(train$price)
Ridge <- glmnet(x = X, y = y, alpha = 0)
RidgeCV <- cv.glmnet(x = X, y = y, alpha = 0, lambda = Ridge$lambda, nfolds = 10)
lambda.ind <- which.min(RidgeCV$cvm)
lambda.best <- Ridge$lambda[lambda.ind]
lambda.best
Ridge$beta[, lambda.ind]


xTrain <- num.feat %>% dplyr::select(-c(price,lprice)) %>% as.matrix()
yTrain <- num.feat$lprice
# Fit model using the best lambda from above
proRidgeT <- glmnet(x = xTrain, y = yTrain, alpha = 0, lambda = lambda.best)
# Use testing data and fitted model to predict
XTest <- test %>%  dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count) %>% as.matrix()
yTest <- log(test$price)
yPredRidge <- proRidgeT$a0 + XTest%*%proRidgeT$beta
# Compute mean-squared prediction error
proMSPE.Ridge <- mean((yTest - yPredRidge@x)^2)
proMSPE.Ridge

```

###Lasso
```{r glmnet}
X <- data.matrix(bnb)
X <- X[,-c(10,2,4,5,6,9,13,14)]
y <- log(bnb$price)

X <- num.feat %>% 
Lasso <- glmnet(x = X, y = y, alpha = 1)


LassoCV <- cv.glmnet(x = X, y = y, alpha = 1, lambda = Lasso$lambda, nfolds = 10)
lambda.indL <- which.min(LassoCV$cvm)
lambda.bestL <- Lasso$lambda[lambda.indL]
lambda.bestL
Lasso$beta[, lambda.indL]

xTrain <- num.feat %>% dplyr::select(-c(price,lprice)) %>% as.matrix()
yTrain <- num.feat$lprice
# Fit model using the best lambda from above
proLassoT <- glmnet(x = xTrain, y = yTrain, alpha = 1, lambda = lambda.best)
# Use testing data and fitted model to predict
XTest <- test %>%  dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count) %>% as.matrix()
yTest <- log(test$price)
yPredLasso <- proLassoT$a0 + XTest%*%proLassoT$beta
# Compute mean-squared prediction error
proMSPE.Lasso <- mean((yTest - yPredLasso@x)^2)
proMSPE.Lasso
```

# Conclusion

There were many considerations to be taken into account when selecting our final model. First, we had to consider the accuracy of the model; if our test error rate was too high, there would be no reason for to implement a poorly-constructed model. Next was model complexity: we could use a model with multiple predictors and parameters, decreasing the error rate of our model, but increasing the variance, or we could contstruct a simpler model with more bias, but lower variability. Choosing a simpler model would allow us to use our model in a larger setting and achieve similar test error rates. We can compare the test error rates of our different models and determine a best model.

## Tree Error Rates

```{r tree_tables, echo = FALSE}
Methods <- c('Tree','Bagging','Boosting', 'Random Forest', 'Reduced Random Forest')
MSPE <- round(c(tree.mspe, bag.mspe, boost.mspe, rf.mspe, rfr.reg.mspe),6)
Misclassification <- round(c(tree.class.misclass, bag.misclass ,boost.misclass, rf.misclass, rfr.misclass), 6)

mspe.tab  <- cbind(Methods,MSPE)
misclass.tab <- cbind(Methods,Misclassification)

kable(list(`MSPE` = mspe.tab, Misclassification  = misclass.tab), 
      caption = 'Error Rates for Tree Models') %>% kable_styling(latex_options = "hold_position")
```


```{r}
Methods <- c("Linear Kernel SVM", "Polynomial Kernel SVM", "Radial Kernel SVM", "KNN")
Misclassification <- round(c(acc.linear, acc.poly, acc.rad, acc.knn), 6)

misclass.tab <- cbind(Methods,Misclassification)

kable(list(`Misclassification`  = misclass.tab), 
      caption = 'Misclassification Rates for SVM and KNN') %>% kable_styling(latex_options = "hold_position")
```

## Final Models and error

# Appendix

```{r ref.label="library", eval = F}

```

Read in the CSV and check the dimensions of the data.
```{r ref.label="read_data", eval = F}

```

Since observations which have a price of 0 will not be useful to our analysis, and are likely to be representative of a bad data point, we will remove these observations.

```{r ref.label="remove_zero", eval = F}

```

Train-test split the data

```{r ref.label="train-test", eval = F}

```

We can get the column names of the columns which contain NA's with the following code:

```{r ref.label="na_cols", eval = F}

```

We can see that there are NA reviews in the `reviews_per_month`, the number of reviews per month. We can also see upon visual inspection `last_review`, the date of the last review the host received, also contains empty values. We will not be using last_review in our analysis, so we will not worry about imputing values here. 

We believe the reason there are NA's in the `reviews_per_month` column is because the hosts have 0 reviews overall. We further explore this claim the below:


```{r ref.label="sum_zero", eval = F}

```

We can see there are no cases where the `number_of_reviews` and `reviews_per_month`. As a result, we will impute 0 where `reviews_per_month` is NA.

```{r ref.label="set_na", eval = F}

```

We can assess the correlation between numeric features with a correlation heatmap:

```{r ref.label="corr_plot", eval = F}

```

```{r ref.label="pairplot1", eval = F}

```

```{r ref.label="pairplot2", eval = F}

```

```{r ref.label="neighbourhood", eval = F}

```

There are 221 neighborhoods covered in the overall data, but only 5 neighbourhood groups. We will further investigate whether we need to use the neighbourhood, or whether we would like to use the negihbourhood groups for simplicity of our model. 

We will determine whether we should use the neigbourhood by seeing if there is a large disparity in mean price by calculating the mean price for the neighbourhood. If there seems to be large disparities within the neighbourhood group for mean pricing, we will attempt to use neighbourhood itself.

```{r ref.label="logprice_hist", eval = F}

```

```{r ref.label="location", eval = F}

```

```{r ref.label="mean_price", eval = F}

```

It does not seem there are any large disparities in pricing, and all the neighbourhood groups seems to be similar to their nearby neighbours. To reduce the complexity of our model, we will use the neighbourhood group.

```{r ref.label="price_hist", eval = F}

```

```{r ref.label="corr_plot", eval = F}

```

```{r ref.label="ols", eval = F}

```

```{r ref.label="logit", eval = F}

```

```{r ref.label="class_tree", eval = F}

```

```{r ref.label="prune_tree", eval = F}

```

```{r ref.label="reg_tree", eval = F}

```

```{r ref.label="rf_class", eval = F}

```

```{r ref.label="rf_reg", eval = F}

```

There still does not seem to be much of an improvement over the regression tree. We can try to re-evaluate the random forest model through cross validation and seeing if we can select important features.

The variable importance plot shows us that `room_type`, `longitude`, `latitude`, and `reviews_per_month` are the most important variables.

```{r ref.label = 'var_imp', eval = F}

```


```{r ref.label="cv_rf", eval = F}

```

As we can see, the cross validation error is the lowest when we use the most predictors. Despite this, There does not seem to be much of a decrease after there are 4 variables in the model, so we will try to fit a model with 4 variables.

We will try fitting the 4 most important variables from the regression random forest, and seeing whether this model is better, or the same, as out more complex model.

```{r ref.label="red_rf", eval = F}

```

By reducing the number of predictors, we were able to slightly increase the MSE, while creating a much simpler model.

Lets try bagging with the smaller subset of variables

```{r ref.label="bag_reg", echo = F}

```

The prediction error is about the same as it is for a random forest.


```{r ref.label="bag_class", eval = F}

```


```{r ref.label="boost_red", eval = F}

```

As with all our other models, this one is about the same. We can try different values of the shrinkage and see if we can find a best model for cross validation error


```{r ref.label="boost_lambda", eval = F}

```

There does not seem to be a discernable lambda from the plot.

```{r ref.label="best_boost", eval = F}

```

```{r ref.label="ntree", eval = F}

```

```{r ref.label="boost_class_lambda", eval = F}

```

```{r ref.label="end_class_boost", eval = F}

```

```{r ref.label="end_reg_boost", eval = F}

```

```{r ref.label="boost_lambda", eval = F}

```

```{r ref.label="boost_lambda", eval = F}

```

```{r ref.label="boost_lambda", eval = F}

```

```{r ref.label="boost_lambda", eval = F}

```
