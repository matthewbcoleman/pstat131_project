---
title: "Airbnb Project"
author: "Matthew Coleman, Austin Mac, Jeff Pittman, and Nick Reyes"
date: "2/28/2020"
output: pdf_document
---

```{r}
library(tidyverse)
library(dplyr)
library(randomForest)
library(class)
library(tree)
#comment
```

Read in the CSV and check the dimensions of the data.
```{r}
bnb <- read.csv('AB_NYC_2019.csv')

bnb <- as_tibble(bnb)

dims <- dim(bnb)

sprintf('Our dataset has %d observations and %d attributes', dims[1],dims[2])
```

Train-test split the data
```{r}
train.ind <- sample(1:nrow(bnb),size = .8*nrow(bnb))


train <- bnb[train.ind,]
test <- bnb[-train.ind,]

```

The response variable for our analysis is going to be `price`, the price per night of the rental. The main predictor variables we are going to explore in this analyses are: `longitude`, `latitude`, `minimum_nights`, `number_of_reviews`, `reviews_per_month`, `neighbourhood`, `neighbourhood_group`, `room_type`, and `calculated_host_listings_count`.


We can get the column names of the columns which contain NA's with the following code:
```{r}
colnames(train)[apply(train, 2, anyNA)]
```

### Need a better description on what reviews per month means

We can see that there are NA reviews in the `reviews_per_month`, the number of reviews per month. We can also see upon visual inspection `last_review`, the date of the last review the host received, also contains empty values. We will not be using last_review in our analysis, so we will not worry about imputing values here. 

We believe the reason there are NA's in the `reviews_per_month` column is because the hosts have 0 reviews overall. We further explore this claim the below:

```{r}
with(train, sum((is.na(reviews_per_month)) & (number_of_reviews!=0)) )
```
We can see there are no cases where the `number_of_reviews` and `reviews_per_month`. As a result, we will impute 0 where `reviews_per_month` is NA.

```{r}
train[is.na(train$reviews_per_month),'reviews_per_month'] <- 0
test[is.na(test$reviews_per_month),'reviews_per_month'] <- 0

sum(is.na(train$reviews_per_month))
sum(is.na(train$reviews_per_month))
```

# Mention what the room types are in the paper when we describe the variables we are using in the report.

```{r}
levels(train$room_type)

n_distinct(bnb$neighbourhood)

n_distinct(train$neighbourhood_group)

```
There are 221 neighborhoods covered in the overall data, but only 5 neighbourhood groups. We will further investigate whether we need to use the neighbourhood, or whether we would like to use the negihbourhood groups for simplicity of our model. 

```{r}
ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood)) + 
  geom_point() + theme(legend.position="none")

ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood_group)) + 

```

We will determine whether we should use the neigbourhood by seeing if there is a large disparity in mean price by calculating the mean price for the neighbourhood. If there seems to be large disparities within the neighbourhood group for mean pricing, we will attempt to use neighbourhood itself.
```{r}
mean_price <- train %>% group_by(neighbourhood) %>% summarise(mean_price = mean(price), 
                                                            latitude = median(latitude), longitude = 
                                                            median(longitude)) 

#plot(mean_price$latitude, mean_price$longitude, col = mean_price$mean_price)

ggplot(data = mean_price, aes(x = latitude, y = longitude, color = mean_price)) + 
  geom_point() + scale_color_gradient(low="blue", high="red")
```
It does not seem there are any large disparities in pricing, and all the neighbourhood groups seems to be similar to their nearby neighbours. To reduce the complexity of our model, we will use the neighbourhood group.

To use a classification of prices, we may want to classify whether a listing will be above the mean *or* median price. We can create this variable by determining whether the price data is skewed. 

```{r}
hist(train$price)
```
As we can see, the prices are highly skewed, with a very small number of high price observations. Some of our models are robust to outliers, so we do not need to filter the outliers unless necessary. Despite this, we will use a binary variable stating whether a price is above the median the median for classification.

```{r}
train$price_above <- ifelse(train$price> median(train$price),1,0)
train$price_above <- as.factor(train$price_above)
test$price_above <- ifelse(test$price> median(test$price),1,0)
test$price_above <- as.factor(test$price_above)
```

### Tree-based Methods

Looking at the latitude and longitude data, we may be able to use a regression tree to determine areas where the price is higher.

```{r}
loc.tree <- tree(price_above~latitude + longitude, data = train)
plot(loc.tree, cex = .65)
text(loc.tree)
```

As we can see from this output, there does not seem to be much difference. Lets try predictions and find out the missclassification rate.

```{r}
location.prediction <- predict(loc.tree, test, type = 'class')
mean(test$price_above !=location.prediction)

#POSSIBLY REMOVE THIS
#plot(cv.tree(loc.tree))

#prune <- prune.tree(loc.tree, best = 4, newdata = test)
#plot(prune)
#text(prune)
```

As we can see, the misclassification rate is pretty high. This may not come to as a surprise because the tree is very small, and covers a large amount of data, leading to over-generalization of the data. Choosing an ensemble tree method may be best for analyzing this dataset.

```{r}
#rest <- randomForest(price_above ~ longitude + latitude + number_of_reviews + neighbourhood_group, data = train)
#importance(rest)
```

# Gender Classification
```{r}
# install.packages("gender")
# install.packages('devtools')
# install_github("ropensci/genderdata")
library(gender)
library(devtools)
library(genderdata)
library(class)
```
Appending column `gender` to bnb.
```{r}
# truncate host name to first word
bnb$host_name <- word(bnb$host_name, 1)
# classify gender based off of host name
gender <- distinct(gender(bnb$host_name, method = "ssa"))
# add gender column to bnb
bnb <- merge(bnb, gender, by.x = "host_name", by.y = "name", all.x = TRUE)
bnb$gender <- as.factor(bnb$gender)
table(bnb$gender)
bnb <- na.omit(bnb)
```

## Gender Train/Test Split
```{r}
g.ID <- sample(1:nrow(bnb), 0.8*nrow(bnb))
g.train <- bnb[g.ID,]
g.test <- bnb[-g.ID,]

xtrain <- g.train[,c("neighbourhood_group", 
                     "latitude", 
                     "longitude", 
                     "room_type", 
                     "price", 
                     "minimum_nights", 
                     "number_of_reviews", 
                     "reviews_per_month", 
                     "calculated_host_listings_count", 
                     "availability_365")]

# crude conversion from factor to numeric for knn method
xtrain$neighbourhood_group <- as.numeric(xtrain$neighbourhood_group)
xtrain$room_type <- as.numeric(xtrain$room_type)

ytrain <- g.train$gender

xtest <- g.test[1:16]
ytest <- g.test$gender
```

## Predicting Gender with KNN
TODO: find best k by cross validation
```{r}
pred.ytrain <- knn(train = xtrain, test = xtrain, cl = ytrain, k = 5)
(conf.matrix <- table(pred = pred.ytrain, obs = ytrain))
sum(diag(conf.matrix))/sum(conf.matrix)
```


