---
title: "Airbnb Project"
author: "Matthew Coleman, Austin Mac, Jeff Pittman, and Nick Reyes"
date: "2/28/2020"
output: 
  pdf_document:
    number_sections: true
---

```{r}
library(tidyverse)
library(dplyr)
library(randomForest)
library(class)
library(tree)
library(gbm)
library(caret)
#comment
```
# Abstract

# Introduction

# Methods

## Data

The dataset we will be using for our analysis is the dataset [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) from Kaggle. This dataset contains the listing activity and metrics for Airbnb in New York City, New York during 2019. There are 48895 observations and 16 attributes to the dataset. The main features we are going to use for our analysis include the following:

* Price: Our main response variable. The price, in dollars, of the listing per night. Log-transformed to normalize distribution. 
* Price Above: Variable created from `price`, `price_above` is a binary variable of signaling whether a listings price is above the median listing price. 1 represents the price being above the median, and 0 represents the price being below the median.
* Neighbourhood Group: The neighbourhood group to which the listing belongs. There are 5 neighbourhood groups in the dataset.
* Neighbourhood: The neighbourhood to which a listing belongs. This is a nested version of neigbhourhood group, with 217 unique neighbourhood groups.
* Latitude: Latitude coordinates of the listing
* Longitude: Longitude coordinates of the listing
* Room Type: The listing space type. Three types: *Entire home/apt*, *Private room*, *Shared room*. 
* Minimum Nights: The mininum amount of nights someone can stay in the listing.
* Number of reviews: The number of reviews for the host.
* Reviews per Month: The number of reviews per month for the host. Formula:$\frac{Number\ of\ Reviews}{12}$.
* Calculated Host Listings Count: The number of listings per host.

## Assumptions. need to do pairplots for all variables

Many of our machine learning methods are very computationally intensive, so we sampled 15% of the entire dataset, and then train-test split the 15% sample into 80% training 20% test dataset. To verify this was a viable practice, we plotted the distribution of our response variable, price, and verified the distribution is the similar to the distribution of the overall dataset. The histogram is very similar, and even contains some of the outliers we can see in the overall dataset, so we assumed our smaller dataset was representative of the population.

## Machine Learning Methods

### Regression Methods

Methods used to predict the price of a listing:



### Classification Methods

Methods used to predict whether a listings price is above the median:

# Analysis and discussion

# Conclusion

Read in the CSV and check the dimensions of the data.
```{r}
bnb <- read.csv('AB_NYC_2019.csv')

bnb <- as_tibble(bnb)

dims <- dim(bnb)

sprintf('Our dataset has %d observations and %d attributes', dims[1],dims[2])
```
Since observations which have a price of 0 will not be useful to our analyses, and are likely to be representative of a bad data point, we will remove these observations.

```{r}
bnb <- bnb[(bnb$price!=0),]
```


Train-test split the data
```{r}
train.ind <- sample(1:nrow(bnb),size = .8*nrow(bnb))
small.ind <- sample(1:nrow(bnb), size = .15*nrow(bnb))


train.big <- bnb[train.ind,]
test.big <- bnb[-train.ind,]

small <- bnb[small.ind,]
train.small <- sample(1:nrow(small), size = .8*nrow(small))
train <- small[train.small,]
test <- small[-train.small,]

```

The response variable for our analysis is going to be `price`, the price per night of the rental. The main predictor variables we are going to explore in this analyses are: `longitude`, `latitude`, `minimum_nights`, `number_of_reviews`, `reviews_per_month`, `neighbourhood`, `neighbourhood_group`, `room_type`, and `calculated_host_listings_count`.


We can get the column names of the columns which contain NA's with the following code:
```{r}
colnames(train)[apply(train, 2, anyNA)]
```

### Need a better description on what reviews per month means

We can see that there are NA reviews in the `reviews_per_month`, the number of reviews per month. We can also see upon visual inspection `last_review`, the date of the last review the host received, also contains empty values. We will not be using last_review in our analysis, so we will not worry about imputing values here. 

We believe the reason there are NA's in the `reviews_per_month` column is because the hosts have 0 reviews overall. We further explore this claim the below:

```{r}
with(train, sum((is.na(reviews_per_month)) & (number_of_reviews!=0)) )
```
We can see there are no cases where the `number_of_reviews` and `reviews_per_month`. As a result, we will impute 0 where `reviews_per_month` is NA.

```{r}
train[is.na(train$reviews_per_month),'reviews_per_month'] <- 0
test[is.na(test$reviews_per_month),'reviews_per_month'] <- 0

sum(is.na(train$reviews_per_month))
sum(is.na(train$reviews_per_month))
```

We can assess the correlation between numeric features with a correlation heatmap:

```{r}
num.feat <- train %>% dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count)
feat.corr <- cor(num.feat)
corrplot::corrplot(feat.corr)
```
As we can see, reviews per month and number of reviews are highly correlated, so we can try to remove one of these variables. We will remove number of reviews.

# Mention what the room types are in the paper when we describe the variables we are using in the report.

```{r}
levels(bnb$room_type)

n_distinct(bnb$neighbourhood)

n_distinct(bnb$neighbourhood_group)

```
There are 221 neighborhoods covered in the overall data, but only 5 neighbourhood groups. We will further investigate whether we need to use the neighbourhood, or whether we would like to use the negihbourhood groups for simplicity of our model. 

```{r}
ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood)) + 
  geom_point() + theme(legend.position="none")

ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood_group)) + 
  geom_point() + theme(legend.position="none")
```

We will determine whether we should use the neigbourhood by seeing if there is a large disparity in mean price by calculating the mean price for the neighbourhood. If there seems to be large disparities within the neighbourhood group for mean pricing, we will attempt to use neighbourhood itself.
```{r}
mean_price <- train %>% group_by(neighbourhood) %>% summarise(mean_price = mean(price), 
                                                            latitude = median(latitude), longitude = 
                                                            median(longitude)) 

#plot(mean_price$latitude, mean_price$longitude, col = mean_price$mean_price)

ggplot(data = mean_price, aes(x = latitude, y = longitude, color = mean_price)) + 
  geom_point() + scale_color_gradient(low="blue", high="red")
```
It does not seem there are any large disparities in pricing, and all the neighbourhood groups seems to be similar to their nearby neighbours. To reduce the complexity of our model, we will use the neighbourhood group.

To use a classification of prices, we may want to classify whether a listing will be above the mean *or* median price. We can create this variable by determining whether the price data is skewed. 

```{r}
hist(train$price)
```
As we can see, the prices are highly skewed, with a very small number of high price observations. Some of our models are robust to outliers, so we do not need to filter the outliers unless necessary. Despite this, we will use a binary variable stating whether a price is above the median the median for classification.

```{r}
train$price_above <- ifelse(train$price> median(train$price),1,0)
train$price_above <- as.factor(train$price_above)
test$price_above <- ifelse(test$price> median(test$price),1,0)
test$price_above <- as.factor(test$price_above)
```

```{r}
#initial glm model
attach(train)
glm.fit <- glm(price_above ~ longitude + latitude,
               data=train, family=binomial)
summary(glm.fit)
fit.probs <- predict(glm.fit, test, type="response")
fit.pred <- rep(0, length(price_above))
fit.pred[fit.probs>.5]=1
table(fit.pred,price_above)
mean(fit.pred!=price_above)
```

```{r}
#initial lda model
library(MASS)
lda.fit <- lda(price_above ~ longitude + latitude +
                 minimum_nights,
               data=train)
lda.fit

yhat <- predict(lda.fit)$class
tr.tbl <- table(obs=train$price_above,pred=yhat)
tr.tbl
1-sum(diag(tr.tbl))/sum(tr.tbl)
```

### Tree-based Methods

Looking at the latitude and longitude data, we may be able to use a regression tree to determine areas where the price is higher.

```{r}
set.seed(123)
loc.tree <- tree(price_above~latitude + longitude, data = train)
plot(loc.tree, cex = .65)
text(loc.tree)

tree
```

As we can see from this output, there does not seem to be much difference. Lets try predictions and find out the missclassification rate.

```{r}
set.seed(123)
location.prediction <- predict(loc.tree, test, type = 'class')
mean(test$price_above !=location.prediction)

#POSSIBLY REMOVE THIS
#plot(cv.tree(loc.tree))

#prune <- prune.tree(loc.tree, best = 4, newdata = test)
#plot(prune)
#text(prune)
```

```{r}
set.seed(123)
tree.reg <- tree(log(price) ~ latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)
reg.prediction <- predict(tree.reg, test)

tree.mspe <- mean((log(test$price)-reg.prediction)^2)
tree.mspe
```

As we can see, the misclassification rate is pretty high. This may not come to as a surprise because the tree is very small, and covers a large amount of data, leading to over-generalization of the data. Choosing an ensemble tree method may be best for analyzing this dataset.

```{r}
set.seed(123)
rf.class <- randomForest(price_above  ~ latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

importance(rf.class)

rf.class.pred <- predict(rf.class,test)
rf.missclass <- mean(test$price_above!=rf.class.pred)

sprintf('The misclassification rate for the classification random forest is %f', rf.missclass)
varImpPlot(rf.class)

```

As we can see, using the model with all the relevant predictors has almost half the missclassification rate as the single tree with longitude and latitude.

```{r}
set.seed(123)
rf.reg <- randomForest(log(price)  ~latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

rf.reg.pred <- predict(rf.reg,test)
rf.mspe <- mean((log(test$price)-rf.reg.pred)^2)

sprintf('The mean squared prediction error for the regression random forest is %f', rf.mspe)
```
There still does not seem to be much of an improvement over the tree for the regression fit on the data. We can try to re-evaluate the random forest model through cross validation and seeing if we can select important features.

```{r}
varImpPlot(rf.reg)
```
Th variable importance plot shows us that `room_type`, `longitude`, `latitude`, and `reviews_per_month` are the most important variables.

```{r}
set.seed(123)
rf.cv.trainx <- train %>% dplyr::select(latitude, longitude, minimum_nights, reviews_per_month,
                                neighbourhood_group, room_type, calculated_host_listings_count)
rf.cv.trainy <- log(train$price)
cv.rf <- rfcv(rf.cv.trainx,rf.cv.trainy, cv.fold = 5)
plot(cv.rf$n.var, cv.rf$error.cv, type = 'b', xlab = 'Number of Variables in Model', ylab = 'Cross-Validation Error')
```
As we can see, the cross validation error is the lowest when we use the most predictors. Despite this, There does not seem to be much of a decrease after there are 4 variables in the model, so we will try to fit a model with 4 variables.

We will try fitting the 4 most important variables from the regression random forest, and seeing whether this model is better, or the same, as out more complex model.

```{r}
set.seed(123)
rf.reduced <- randomForest(log(price)  ~ latitude + longitude + reviews_per_month + room_type, data = train, mportance = TRUE)

rf.reg.red <- predict(rf.reduced,test)
rf.red.mspe <- mean((log(test$price)-rf.reg.red)^2)

sprintf('The mean squared prediction error for the regression random forest is %f', rf.red.mspe)
```
By reducing the number of predictors, we were able to slightly increase the MSE, while creating a much simpler model.

# Bagging

Lets try bagging with the smaller subset of variables
```{r}
set.seed(123)
bag.reg <- randomForest(log(price)  ~ latitude + longitude + reviews_per_month +  room_type,
                     data = train, mtry = 4 , importance = TRUE)

bag.reg.pred <- predict(bag.reg,test)
bag.mspe <- mean((log(test$price)-bag.reg.pred)^2)

sprintf('The mean squared prediction error for bagging is %f', bag.mspe)
```
The prediction error is about the same as it is for a random forest.

# Boosting

```{r}
boost.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, cv.folds = 5, distribution = 'gaussian')
boost.pred <- predict(boost.mod, test, n.trees = 1000)

boost.mspe <- mean((log(test$price)-boost.pred)^2)

sprintf('The mean squared prediction error for bagging is %f', boost.mspe)
```

As with all our other models, this one is about the same. We can try different values of the shrinkage and see if we can find a best model for cross validation error

```{r}
lambdas <- seq(0,.5, .01)
b.mspe.list <- NULL

for(lambda in lambdas){
  boost.l.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, shrinkage = lambda, distribution = 'gaussian')
  boost.l.pred <- predict(boost.l.mod, test, n.trees = 1000)

  b.mspe.list <- append(b.mspe.list,mean((log(test$price)-boost.l.pred)^2))
}
plot(lambdas,b.mspe.list, type = 'b')
```
There does not seem to be a discernable lambda from the plot.

```{r}
best.lambda <- lambdas[which.min(b.mspe.list)]

best.boost <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, distribution = 'gaussian')
best.boost <- predict(boost.mod, test, n.trees = 1000)

b.boost.mspe <- mean((log(test$price)-best.boost)^2)

sprintf('The mean squared prediction error for bagging with the optimal lambda is is %f', b.boost.mspe)
```
The mean squared prediction error has not improved with the best lambda.

```{r}
tree.err <- NULL

ntrees <- list(500,1000,1500,2000,2500)

for(ntree in ntrees){
  boost.t.mod <- gbm(log(price) ~ latitude + longitude + reviews_per_month +  room_type, data = train,
               n.trees = ntree, shrinkage = best.lambda, distribution = 'gaussian')
  boost.t.pred <- predict(boost.t.mod, test, n.trees = ntree)

  tree.err <- append(tree.err,mean((log(test$price)-boost.t.pred)^2))
}
tree.err
```


```{r}
class.boost <- gbm(price_above ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000,  distribution = 'bernoulli')
class.boost <- predict(boost.mod, test, n.trees = 1000)

boost.class.pred <- predict(rf.class,test)
boost.missclass <- mean(test$price_above!=rf.class.pred)
boost.missclass
```


# SVM
```{r}
library(e1071)
train$price_above <- as.factor(train$price_above)
# plot(train$latitude, train$longitude, col = as.numeric(train$price_above) + 1)

svmfit <- svm(price_above ~ latitude+longitude, data = train, cost = 10, kernel = "linear", scale = FALSE)
plot(svmfit, train[,c("price_above", "latitude", "longitude")], symbolPalette = terrain.colors(2))
```

```{r}
svmpoly <- svm(price_above ~ latitude+longitude, data = train, cost = 10, kernel = "polynomial", scale = FALSE)
plot(svmpoly, train[,c("price_above", "latitude", "longitude")], symbolPalette = terrain.colors(2))
```

```{r}
svmrad <- svm(price_above ~ latitude+longitude, data = train, cost = 10, kernel = "radial", scale = FALSE)
plot(svmrad, train[,c("price_above", "latitude", "longitude")], symbolPalette = terrain.colors(2))
```

```{r}
svmrad <- svm(price_above ~ latitude+longitude, data = train, cost = .1, kernel = "radial", scale = FALSE)
plot(svmrad, train[,c("price_above", "latitude", "longitude")], symbolPalette = terrain.colors(2))
```

```{r}
svmrad <- svm(price_above ~ latitude+longitude, data = train, cost = 100, kernel = "radial", scale = FALSE)
plot(svmrad, train[,c("price_above", "latitude", "longitude")], symbolPalette = terrain.colors(2))
```

Tuning
```{r, eval=FALSE}
tune.out <- tune(svm, price_above ~ latitude+longitude, data = train, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
```


# Gender Classification
```{r}
# install.packages("gender")
# install.packages('devtools')
# install_github("ropensci/genderdata")
library(gender)
library(devtools)
library(genderdata)
library(class)
```
Appending column `gender` to bnb.
```{r, evaL=FALSE}
# truncate host name to first word
bnb$host_name <- word(bnb$host_name, 1)
# classify gender based off of host name
gender <- distinct(gender(bnb$host_name, method = "ssa"))
# add gender column to bnb
bnb <- merge(bnb, gender, by.x = "host_name", by.y = "name", all.x = TRUE)
bnb$gender <- as.factor(bnb$gender)
table(bnb$gender)
bnb <- na.omit(bnb)
```

## Gender Train/Test Split
```{r, eval=FALSE}
g.ID <- sample(1:nrow(bnb), 0.8*nrow(bnb))
g.train <- bnb[g.ID,]
g.test <- bnb[-g.ID,]

xtrain <- g.train[,c("neighbourhood_group", 
                     "latitude", 
                     "longitude", 
                     "room_type", 
                     "price", 
                     "minimum_nights", 
                     "number_of_reviews", 
                     "reviews_per_month", 
                     "calculated_host_listings_count", 
                     "availability_365")]

# crude conversion from factor to numeric for knn method
xtrain$neighbourhood_group <- as.numeric(xtrain$neighbourhood_group)
xtrain$room_type <- as.numeric(xtrain$room_type)

ytrain <- g.train$gender

xtest <- g.test[1:16]
ytest <- g.test$gender
```

## Predicting Gender with KNN
TODO: find best k by cross validation
```{r, eval=FALSE}
pred.ytrain <- knn(train = xtrain, test = xtrain, cl = ytrain, k = 5)
(conf.matrix <- table(pred = pred.ytrain, obs = ytrain))
sum(diag(conf.matrix))/sum(conf.matrix)
```


