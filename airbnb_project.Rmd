---
title: "Airbnb Project"
author: "Matthew Coleman, Austin Mac, Jeff Pittman, and Nick Reyes"
date: "2/28/2020"
output: 
  pdf_document:
    number_sections: true
    fig_caption: yes
    includes:  
      in_header: preamble-latex.tex
---

```{r library, echo = F}
# Anything below here before the abstract has to be here so we can run code in the analysis and have output
library(tidyverse)
library(dplyr)
library(randomForest)
library(class)
library(tree)
library(gbm)
library(caret)
library(rpart.plot)
library(rattle)
library(knitr)
library(fastAdaboost)
library(ggpubr)
```

```{r read_data, echo = F}
bnb <- read.csv('AB_NYC_2019.csv')

bnb <- as_tibble(bnb)

dims <- dim(bnb)

sprintf('Our dataset has %d observations and %d attributes', dims[1],dims[2])
```

```{r remove_zero, echo = F}
bnb <- bnb[(bnb$price!=0),]
```

```{r train-test, echo = F}
set.seed(123)
train.ind <- sample(1:nrow(bnb),size = .8*nrow(bnb))
small.ind <- sample(1:nrow(bnb), size = .15*nrow(bnb))


train.big <- bnb[train.ind,]
test.big <- bnb[-train.ind,]

small <- bnb[small.ind,]
train.small <- sample(1:nrow(small), size = .8*nrow(small))
train <- small[train.small,]
test <- small[-train.small,]

```

```{r na_cols, echo = F}
colnames(train)[apply(train, 2, anyNA)]
```

```{r sum_zero, echo = F}
with(train, sum((is.na(reviews_per_month)) & (number_of_reviews!=0)) )
```

```{r set_na, echo = F}
train[is.na(train$reviews_per_month),'reviews_per_month'] <- 0
test[is.na(test$reviews_per_month),'reviews_per_month'] <- 0

sum(is.na(train$reviews_per_month))
sum(is.na(train$reviews_per_month))
```

```{r price_above, echo = F}
train$price_above <- ifelse(train$price> median(train$price),1,0)
#train$price_above <- as.factor(train$price_above)
test$price_above <- ifelse(test$price> median(test$price),1,0)
#test$price_above <- as.factor(test$price_above)
```

```{r, echo = F}
num.feat <- train %>% dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count, price) 
num.feat$lprice <- log(num.feat$price)
feat.corr <- cor(num.feat)
corrplot::corrplot(feat.corr, main = 'Feature Correlations')
```

```{r pairplot, echo = F}
pairs(num.feat)
```

```{r neighbourhood, echo = F}
levels(bnb$room_type)

levels(bnb$neighbourhood_group)

n_distinct(bnb$neighbourhood)

n_distinct(bnb$neighbourhood_group)

```

# Abstract

# Introduction



# Methods

## Data

The dataset we will be using for our analysis is the dataset [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) from Kaggle. This dataset contains the listing activity and metrics for Airbnb in New York City, New York during 2019. There are 48895 observations and 16 attributes to the dataset. The main features we are going to use for our analysis include the following:

* Price: Our main response variable. The price, in dollars, of the listing per night. Log-transformed to normalize distribution. 

```{r, echo = FALSE, fig.cap = 'Log-transformed Price'}
hist(log(train$price), main='Histogram of log(Price)', xlab = 'log(Price)')
```

* Price Above: Variable created from `price`, `price_above` is a binary variable of signaling whether a listings price is above the median listing price. 1 represents the price being above the median, and 0 represents the price being below the median.
* Neighbourhood: Categorical variable of the neighbourhood to which a listing belongs. This is a nested version of neigbhourhood group, with 221 unique neighbourhood groups.
* Neighbourhood Group: Factor variable of the neighbourhood group to which the listing belongs. There are 5 neighbourhood groups in the dataset.
  * Plots of both neigbourhood and neighbourhood group are shown below: 
  
    ```{r location, fig.width = 6.5, echo = F, fig.cap = 'Neighbourhood and neighbourhood group}
n <- ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood)) + 
  geom_point() + theme(legend.position="none") + xlab('Latitude') + ylab('Longitude') +
  ggtitle('New York City Airbnb Listing Neighbourhood')

ng <- ggplot(data = train, aes(x = latitude, y = longitude, color = neighbourhood_group)) + 
  geom_point() + theme(legend.position="none") + xlab('Latitude') + ylab('Longitude') +
  ggtitle('New York City Airbnb Listing Neighbourhood Groups')

ggarrange(n, ng, nrow = 1, ncol = 2)
```


```{r mean_price, echo = F, fig.cap = 'Justification for using neighbourhood group'}
mean_price <- train %>% group_by(neighbourhood) %>% summarise(mean_price = mean(price), 
                                                            latitude = median(latitude), longitude = 
                                                            median(longitude)) 

#plot(mean_price$latitude, mean_price$longitude, col = mean_price$mean_price)

ggplot(data = mean_price, aes(x = latitude, y = longitude, color = mean_price)) + 
  geom_point() + scale_color_gradient(low="blue", high="red", name = 'Mean Price (USD)') + xlab('Latitude') + 
  ylab('Longitude') + ggtitle('Mean Price by Neighborhood')
```

* Latitude: Latitude coordinates of the listing.
* Longitude: Longitude coordinates of the listing.
* Room Type: The listing space type. Three types: *Entire home/apt*, *Private room*, *Shared room*. 
* Minimum Nights: The mininum amount of nights someone can stay in the listing.
* Number of reviews: The number of reviews for the host.
* Reviews per Month: The number of reviews per month for the host. Formula:$\frac{Number\ of\ Reviews}{Months\ Listed}$.
* Calculated Host Listings Count: The number of listings per host.

All atributes were complete with the exception of `last_review`, which has the date of the last review, and `reviews_per_month`. Upon further exploration, the reviews per month feature was NA only when the host had no reviews. This resulted in us imputing 0's for NA values in the reviews per month column. Because the date of last review was unimportant to our analyses, we did not impute values for this column.

### Assumptions.

Many of our machine learning methods are very computationally intensive, so we sampled 15% of the entire dataset, and then train-test split the 15% sample into 80% training 20% test dataset. To verify this was a viable practice, we plotted the distribution of our response variable, price, and verified the distribution is the similar to the distribution of the overall dataset. The histogram is very similar, and even contains some of the outliers we can see in the overall dataset, so we assumed our smaller dataset was representative of the population.

```{r price_hist, echo = F, fig.cap= 'Training Data Histogram'}
par(mfrow = c(1,2))
hist(bnb$price, main='Histogram of Overall Price', xlab = 'Price (USD)')
hist(train$price, main='Histogram of Training Price', xlab = 'Price (USD)')
```

We assessed the correlation between our variables with a correlation heatmap:

```{r corr_plot, echo = F}
num.feat <- train %>% dplyr::select(longitude, latitude, minimum_nights, number_of_reviews, 
                                    reviews_per_month, calculated_host_listings_count, price) 
num.feat$lprice <- log(num.feat$price)
feat.corr <- cor(num.feat)
corrplot::corrplot(feat.corr, main = 'Feature Correlations')
```

Reviews per month and number of reviews were highly correlated, so we decided to remove number of reviews to account for collinearity.

### Sample Sizes

Our overall dataset is 48895. Taking the proposed 15% split on the data left us with an overall dataset of 7332 observations. The 80/20 train-test split left us with 5865 training samples and 1467 test samples.

## Machine Learning Methods

### Regression Methods

Methods used to predict the price of a listing:

* Ridge Regression:

  * Constraint optimization on the least squares criterion:
  
$$\hat{\beta}_{ridge} = \underset{\beta}{argmin}[||Y-XB||^2 + \lambda\sum_{j=1}^p\beta_j^2] $$

* Lasso Regression: 

  * Constraint optimization and model selection on the least squares criterion:
  
$$\hat{\beta}_{ridge} = \underset{\beta}{argmin}[\ ||Y-XB||^2 + \lambda\sum_{j=1}^p|\beta_j|\ ]$$

By using these two methods, we can try to reduce our estimates for the linear model by imposing some Bias on our estimates for $\beta$. Another benefit of using Lasso regression is that we can also perform model selection, making a simpler model. For choice of an optimal $\lambda$, we will use cross validation. 

* Tree Methods

  * Individual Trees: To compare the efficacy of ensemble tree methods, we will fit an individual regression tree on longitude and latitude, and then one tree on all variables of interest.
  
  * Bagging: We will fit an ensemble tree method which will grow large trees on bootstrapped data, resulting in high variance low bias. All of these trees predictions will be averaged to give the final prediction.
  
  * Random Forest: We will create multiple decision trees similar to bagging, but try to decorrelate each of the bootstrap trees through selecting $m=\frac{p}{3}$ variables. 
  
  * Boosting: We will fit multiple (weak) trees sequentially, grown on information from the previously grown tree. Final prediction is a weighted prediction of the weak learners.
  

### Classification Methods

Methods used to predict whether a listings price is above the median:

* Logistic Regression
* LDA
* QDA

* Tree Methods

  * Individual Trees: To compare the efficacy of ensemble tree methods, we will fit an individual classification tree on longitude and latitude, and then one tree on all variables of interest.
  
  * Bagging: We will fit an ensemble tree method which will grow large trees on bootstrapped data, resulting in high variance low bias. All of these trees predictions will be chosen by majority voting for the final prediction.
  
  * Random Forest: We will create multiple decision trees similar to bagging, but try to decorrelate each of the bootstrap trees through selecting $m=\sqrt{p}$ variables. Final predictions will be through majority voting. 
  
  * Boosting: We will fit multiple (weak) trees sequentially, grown on information from the previously grown tree. Final prediction is a weighted of the weak learners 
  
* SVM

# Analysis and Discussion

```{r}
set.seed(123)
ols.price <- lm(log(price) ~ latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)
ols.pred <- predict(ols.price, test)

ols.mspe <- mean((log(test$price)-ols.pred)^2)
ols.mspe

```


```{r}
#initial LR model
set.seed(123)
attach(train)
logit.fit <- glm(price_above ~ 
                 longitude 
               + latitude
               + minimum_nights
               + calculated_host_listings_count
               + availability_365
               + reviews_per_month
               + room_type
               + neighbourhood_group
              ,
               data=train, 
               family=binomial('logit'))

fit.probs <- predict(logit.fit, test, type="response")
fit.pred <- rep(0, length(price_above))
fit.pred[fit.probs>.5]=1
table(fit.pred,price_above)
mean(fit.pred!=price_above)
```

```{r}
#initial lda model
library(MASS)
lda.fit <- lda(price_above ~ 
                 longitude 
               + latitude
               + minimum_nights
               + calculated_host_listings_count
               + availability_365
               + reviews_per_month
               + room_type
               + neighbourhood_group
               ,
               data=train)
lda.fit

yhat <- predict(lda.fit)$class
tr.tbl <- table(obs=train$price_above,pred=yhat)
tr.tbl
1-sum(diag(tr.tbl))/sum(tr.tbl)
```

``` {r}
qda.fit <- qda(price_above ~ longitude + latitude
               + minimum_nights
               + calculated_host_listings_count
               + availability_365
               + reviews_per_month
               + room_type
               + neighbourhood_group
               ,
               data=train)
qda.fit

yhat <- predict(qda.fit)$class
tr.tbl <- table(obs=train$price_above,pred=yhat)
tr.tbl
1-sum(diag(tr.tbl))/sum(tr.tbl)
```

### Tree-based Methods

Looking at the latitude and longitude data, we may be able to use a regression tree to determine areas where the price is higher.

```{r}
set.seed(123)
class.tree <- rpart(as.factor(price_above)~latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)

fancyRpartPlot(class.tree, digits = 6, main = 'Classification Tree for Price Above Median', sub = '')
```

As we can see from this output, there does not seem to be much difference. Lets try predictions and find out the missclassification rate.

```{r}
set.seed(123)
tree.class.prediction <- predict(class.tree, test, type = 'class')
tree.class.missclass <- mean(test$price_above !=tree.class.prediction)

#Pruning tree did not improve tree
#plot(cv.tree(loc.tree))

#prune <- prune.tree(loc.tree, best = 4, newdata = test)
#plot(prune)
#text(prune)
```

```{r}
set.seed(123)
tree.reg <- rpart(log(price) ~ latitude + longitude + minimum_nights + reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count, data = train)

fancyRpartPlot(tree.reg, digits = 6,  sub = '')
```

```{r}
set.seed(123)
reg.prediction <- predict(tree.reg, test)

tree.mspe <- mean((log(test$price)-reg.prediction)^2)
tree.mspe
```

As we can see, the misclassification rate is pretty high. This may not come to as a surprise because the tree is very small, and covers a large amount of data, leading to over-generalization of the data. Choosing an ensemble tree method may be best for analyzing this dataset.

```{r}
set.seed(123)
rf.class <- randomForest(price_above  ~ latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

randomForest::importance(rf.class)

rf.class.pred <- predict(rf.class,test)
rf.missclass <- mean(test$price_above!=rf.class.pred)

sprintf('The misclassification rate for the classification random forest is %f', rf.missclass)
varImpPlot(rf.class)

```

As we can see, using the model with all the relevant predictors has almost half the missclassification rate as the single tree with longitude and latitude.

```{r}
set.seed(123)
rf.reg <- randomForest(log(price) ~ latitude + longitude + minimum_nights +  reviews_per_month +
                       neighbourhood_group +  room_type + calculated_host_listings_count,
                     data = train, mportance = TRUE)

rf.reg.pred <- predict(rf.reg,test)
rf.mspe <- mean((log(test$price)-rf.reg.pred)^2)

sprintf('The mean squared prediction error for the regression random forest is %f', rf.mspe)
```
There still does not seem to be much of an improvement over the tree for the regression fit on the data. We can try to re-evaluate the random forest model through cross validation and seeing if we can select important features.

```{r}
varImpPlot(rf.reg)
```

The variable importance plot shows us that `room_type`, `longitude`, `latitude`, and `reviews_per_month` are the most important variables.

```{r}
# set.seed(123)
# rf.cv.trainx <- train %>% dplyr::select(latitude, longitude, minimum_nights, reviews_per_month,
#                                 neighbourhood_group, room_type, calculated_host_listings_count)
# rf.cv.trainy <- log(train$price)
# cv.rf <- rfcv(rf.cv.trainx,rf.cv.trainy, cv.fold = 5)
# plot(cv.rf$n.var, cv.rf$error.cv, type = 'b', xlab = 'Number of Variables in Model', ylab = 'Cross-Validation Error')
```
As we can see, the cross validation error is the lowest when we use the most predictors. Despite this, There does not seem to be much of a decrease after there are 4 variables in the model, so we will try to fit a model with 4 variables.

We will try fitting the 4 most important variables from the regression random forest, and seeing whether this model is better, or the same, as out more complex model.

```{r}
set.seed(123)
rf.reduced <- randomForest(log(price)  ~ latitude + longitude + reviews_per_month + room_type, data = train, mportance = TRUE)

rf.reg.red <- predict(rf.reduced,test)
rf.red.mspe <- mean((log(test$price)-rf.reg.red)^2)

sprintf('The mean squared prediction error for the regression random forest is %f', rf.red.mspe)
```
By reducing the number of predictors, we were able to slightly increase the MSE, while creating a much simpler model.

```{r}
set.seed(123)
rf.red.class <- randomForest(log(price)  ~latitude + longitude +  reviews_per_month +
                       room_type , data = train, mportance = TRUE)

rfr.class.pred <- predict(rf.reg,test)
rfr.missclass <- mean(test$price_above!=rf.class.pred)

sprintf('The misclassification rate for the classification random forest is %f', rfr.missclass)
```

# Bagging

Lets try bagging with the smaller subset of variables
```{r}
set.seed(123)
bag.reg <- randomForest(log(price)  ~ latitude + longitude + reviews_per_month +  room_type,
                     data = train, mtry = 4 , importance = TRUE)

bag.reg.pred <- predict(bag.reg,test)
bag.mspe <- mean((log(test$price)-bag.reg.pred)^2)

sprintf('The mean squared prediction error for bagging is %f', bag.mspe)
```
The prediction error is about the same as it is for a random forest.

```{r}
set.seed(123)
bag.class <- randomForest(price_above ~ latitude + longitude + reviews_per_month +  room_type,
                     data = train, mtry = 4 , importance = TRUE)

bag.class.pred <- predict(bag.class,test)
bag.missclass <- mean(test$price_above!=bag.class.pred)

sprintf('The misclassification rate for the classification random forest is %f', bag.missclass)
```

## Boosting

```{r}
set.seed(123)
boost.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, cv.folds = 5, distribution = 'gaussian')
boost.pred <- predict(boost.mod, test, n.trees = 1000)

boost.mspe <- mean((log(test$price)-boost.pred)^2)

sprintf('The mean squared prediction error for bagging is %f', boost.mspe)
```

As with all our other models, this one is about the same. We can try different values of the shrinkage and see if we can find a best model for cross validation error

```{r}
set.seed(123)
lambdas <- seq(0,.5, .01)
b.mspe.list <- NULL

for(lambda in lambdas){
  boost.l.mod <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, shrinkage = lambda, distribution = 'gaussian')
  boost.l.pred <- predict(boost.l.mod, test, n.trees = 1000)

  b.mspe.list <- append(b.mspe.list,mean((log(test$price)-boost.l.pred)^2))
}
plot(lambdas,b.mspe.list, type = 'b', ylab = 'Boosted MSPE', xlab = 'Lambda', 
     main = 'MSPE vs. Lambdas')
```


There does not seem to be a discernable lambda from the plot.

```{r}
set.seed(123)
best.lambda <- lambdas[which.min(b.mspe.list)]

best.boost <- gbm(log(price) ~ latitude + longitude +   reviews_per_month +  room_type, data = train,
                 n.trees = 1000, distribution = 'gaussian')
best.boost <- predict(boost.mod, test, n.trees = 1000)

b.boost.mspe <- mean((log(test$price)-best.boost)^2)

sprintf('The mean squared prediction error for bagging with the optimal lambda is is %f', b.boost.mspe)
```
The mean squared prediction error has not improved with the best lambda.

```{r}
tree.err <- NULL

ntrees <- list(500,1000,1500,2000,2500)

for(ntree in ntrees){
  boost.t.mod <- gbm(log(price) ~ latitude + longitude + 
                       reviews_per_month +  room_type, data = train,
               n.trees = ntree, shrinkage = best.lambda, distribution = 'gaussian')
  boost.t.pred <- predict(boost.t.mod, test, n.trees = ntree)

  tree.err <- append(tree.err,mean((log(test$price)-boost.t.pred)^2))
}
#tree.err

plot(ntrees, tree.err, type = 'b', ylab = 'Boosted MSPE', xlab = 'Trees', 
     main = 'MSPE vs. Number of Trees')
```

```{r}
set.seed(123)
lambdas <- seq(0,.5, .01)
b.miss.list <- NULL

for(lambda in lambdas){
  boost.m.mod <- gbm(price_above ~ latitude + longitude +   
                       reviews_per_month +  room_type, data = train,
                     n.trees = 1000, shrinkage = lambda, distribution = 'bernoulli')
  boost.m.pred <- predict(boost.m.mod, test, n.trees = 1000)

  b.miss.list <- append(b.miss.list, mean(test$price_above!=boost.m.pred))
}
plot(lambdas, b.miss.list, type = 'b', ylab = 'Boosted MSPE', xlab = 'Lambda', 
     main = 'Missclass. vs. Lambdas')
```

```{r}
set.seed(123)
class.boost <- gbm(price_above ~ latitude + longitude +   
                     reviews_per_month +  room_type, data = train, n.trees = 1000)
boost.class.pred<- predict(class.boost, test, n.trees = 1000)

boost.missclass <- mean(test$price_above!=boost.class.pred)
boost.missclass
```

All The tables for tree methods put together.

```{r tree_tables, echo = FALSE,  fig.cap = ""}
Methods <- c('Tree','Bagging','Boosting', 'Random Forest', 'Reduced Random Forest')
MSPE <- round(c(tree.mspe, bag.mspe, boost.mspe, rf.mspe, rf.red.mspe),6)
Missclassification <- round(c(tree.class.missclass, bag.missclass ,boost.missclass, rf.missclass, rfr.missclass), 6)

mspe.tab  <- cbind(Methods,MSPE)
missclass.tab <- cbind(Methods,Missclassification)

kable(list(`MSPE` = mspe.tab, Missclassification  = missclass.tab))
```


# SVM
Fitting SVM models off of longitude and latitude. 
```{r}
library(e1071)
train$price_above <- as.factor(train$price_above)

plot(rbind(train, test)$latitude, 
     rbind(train,test)$longitude, 
     col = rbind(train, test)$price_above, 
     main = "Price by Location",
     xlab = "Latitude",
     ylab = "Longitude")
```


## Best Linear Kernel SVM
* Misclass. Rate: 0.1792774
* Cost Parameter: 0.09
* Support Vectors: 4598
```{r}
# determine approximate best cost parameter
# tune.linear <- tune(svm, price_above ~ latitude+longitude, data = train, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
# increase precision of cost parameter
# tune.linear <- tune(svm, price_above ~ longitude
#                     +latitude
#                     +neighbourhood
#                     +minimum_nights
#                     +room_type
#                     +minimum_nights
#                     +number_of_reviews
#                     +calculated_host_listings_count
#                     +availability_365, data = train, kernel = "linear", ranges = list(cost = seq(.05, .15, .01)))
best.linear <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood
                    +minimum_nights
                    +room_type
                    +minimum_nights
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "linear", cost = 0.06)
pred.linear <- predict(best.linear, test)

plot(best.linear, test[,c("price_above","longitude", "latitude")])

(MSE.linear <-mean((as.numeric(test$price_above) - as.numeric(pred.linear))^2))

# confusion matrix
(conf.linear <- table(obs = test$price_above, pred = pred.linear))
(acc <- 1 - sum(diag(conf.linear)/sum(conf.linear)))

```

## Best Polynomial Kernel SVM
* Misclass. Rate: 0.1785958
* Cost Parameter: 10
* Degree: 3
* Support Vectors: 5737
```{r}
tune.poly <- tune(svm, price_above ~ longitude
                    +latitude
                    +neighbourhood
                    +minimum_nights
                    +room_type
                    +minimum_nights
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "polynomial", ranges = list(cost = c(9, 10, 11)))
best.poly <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood
                    +minimum_nights
                    +room_type
                    +minimum_nights
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernal = "polynomial", cost = 10)
pred.poly <- predict(best.poly, test)
plot(best.poly, test[,c("price_above", "longitude", "latitude")])
(MSE.poly <- mean((as.numeric(test$price_above) - as.numeric(pred.poly))^2))

# confusion matrix
(conf.poly <- table(obs = test$price_above, pred = pred.poly))
(acc <- 1 - sum(diag(conf.poly)/sum(conf.poly)))
```

## Best Radial Kernel SVM
* MSE: 0.1785958
* Cost Parameter: 14
* Support Vectors: 3615
```{r}
# tune.rad <- tune(svm, price_above ~ longitude+latitude, data = train, kernel = "radial", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
tune.rad <- tune(svm, price_above ~ longitude
                    +latitude
                    +neighbourhood
                    +minimum_nights
                    +room_type
                    +minimum_nights
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "radial", ranges = list(cost = seq(8, 11, 1)))
best.rad <- svm(price_above ~ longitude
                    +latitude
                    +neighbourhood
                    +minimum_nights
                    +room_type
                    +minimum_nights
                    +number_of_reviews
                    +calculated_host_listings_count
                    +availability_365, data = train, kernel = "radial", cost = 8)
plot(best.rad, test[,c("price_above", "latitude", "longitude")])
pred.rad <- predict(best.rad, test)
(conf.rad <- table(obs=test$price_above, pred=pred.rad))
(acc <- 1 - sum(diag(conf.rad)/sum(conf.rad)))
```

## Predicting Gender with KNN
TODO: find best k by cross validation
```{r, eval=FALSE}
pred.ytrain <- knn(train = xtrain, test = xtrain, cl = ytrain, k = 5)
(conf.matrix <- table(pred = pred.ytrain, obs = ytrain))
sum(diag(conf.matrix))/sum(conf.matrix)
```

# Conclusion

# Appendix

```{r ref.label="library", eval = F}

```

Read in the CSV and check the dimensions of the data.
```{r ref.label="read_data", eval = F}

```

Since observations which have a price of 0 will not be useful to our analysis, and are likely to be representative of a bad data point, we will remove these observations.

```{r ref.label="remove_zero", eval = F}

```

Train-test split the data

```{r ref.label="train-test", eval = F}

```

We can get the column names of the columns which contain NA's with the following code:

```{r ref.label="na_cols", eval = F}

```

We can see that there are NA reviews in the `reviews_per_month`, the number of reviews per month. We can also see upon visual inspection `last_review`, the date of the last review the host received, also contains empty values. We will not be using last_review in our analysis, so we will not worry about imputing values here. 

We believe the reason there are NA's in the `reviews_per_month` column is because the hosts have 0 reviews overall. We further explore this claim the below:


```{r ref.label="sum_zero", eval = F}

```

We can see there are no cases where the `number_of_reviews` and `reviews_per_month`. As a result, we will impute 0 where `reviews_per_month` is NA.

```{r ref.label="set_na", eval = F}

```

We can assess the correlation between numeric features with a correlation heatmap:

```{r ref.label="corr_plot", eval = F}

```

```{r ref.label="pairplot", eval = F}

```

```{r ref.label="neighbourhood", eval = F}

```

There are 221 neighborhoods covered in the overall data, but only 5 neighbourhood groups. We will further investigate whether we need to use the neighbourhood, or whether we would like to use the negihbourhood groups for simplicity of our model. 

We will determine whether we should use the neigbourhood by seeing if there is a large disparity in mean price by calculating the mean price for the neighbourhood. If there seems to be large disparities within the neighbourhood group for mean pricing, we will attempt to use neighbourhood itself.

```{r ref.label="location", eval = F}

```

It does not seem there are any large disparities in pricing, and all the neighbourhood groups seems to be similar to their nearby neighbours. To reduce the complexity of our model, we will use the neighbourhood group.

```{r ref.label="price_hist", eval = F}

```

```{r ref.label="library", eval = F}

```

```{r ref.label="library", eval = F}

```

```{r ref.label="library", eval = F}

```


```{r ref.label="library", eval = F}

```